#!/usr/bin/python 
#
# FileMap - http://mfisk.github.com/filemap
#
# Public Domain License:
#
# This program was prepared by Los Alamos National Security, LLC at
# Los Alamos National Laboratory (LANL) under contract
# No. DE-AC52-06NA25396 with the U.S. Department of Energy (DOE). All
# rights in the program are reserved by the DOE and Los Alamos
# National Security, LLC.  Permission is granted to the public to copy
# and use this software without charge, provided that this Notice and
# any statement of authorship are reproduced on all copies.  Neither
# the U.S. Government nor LANS makes any warranty, express or implied,
# or assumes any liability or responsibility for the use of this
# software.
#
# Author: Mike Fisk <mfisk@lanl.gov>
# Other Guy: Alex Brugh <abrugh@lanl.gov>
#

import ConfigParser
import copy
import base64
import cPickle
import cStringIO
import errno
import fcntl
import glob
import grp
import math
import optparse
import os
import pwd
import pydoc
import random
import re
import select
import shlex
import shutil
import signal
import socket
import socket
import stat
import string
import subprocess
import sys
import tempfile
import time
import traceback
import urlparse

try:
   import hashlib # New for python 2.5
   sha = hashlib.sha1
except:
   import sha
   sha = sha.sha

# Globals
Verbose = 0
RegressionTest = False
Locations = None

try:
   select.poll
except:
   print >>sys.stderr, "select.poll() not implemented.  If MacOS, use DarwinPorts python."
   sys.exit(-1)

nonalphanumre = re.compile('\W')

def printableHash(s):
   h = sha(s).digest()
   h = base64.b64encode(h, '+_')  # Normal b64 but / is _
   h = h.rstrip('\n\r =')
   return h

me = socket.gethostname().split('.')[0]

def isremote(hname):
   if not hname:
      return False

   hname = hname.split('.')[0]
   if me == hname:
      return False
   else:
      return True

nonalphanumre = re.compile('\W')

def escape(s):
   (s, num) = nonalphanumre.subn(lambda m: "=%02X" % ord(m.group()), string.join(s))
   return s

quoprire = re.compile('=([0-9a-fA-F]{2})')

def lsUnescapeChr(m):
   c = m.group(1).decode('hex')
   if c == '/':
      c = '\\/'
   if c == '\\':
      c = '\\\\'
   return c

def tabulate(lst, width, ncols=None):
   if not lst:
      return None
   smallest = min([len(l) for l in lst])

   if ncols == None:
      # Initial case, start with each column being as wide as narrowest element
      ncols = (width+2) / (smallest+2)
      ncols = max(1, ncols)

   # See if we can build a table with this many columns
   minwidths = [smallest] * ncols
   nrows = int(math.ceil(len(lst)*1.0 / ncols))
   for c in range(0, ncols):
      colwidth = minwidths[c]
      for r in range(0, nrows):
         i = c * nrows + r
         if i > (len(lst)-1): break
         lel = len(lst[c*nrows + r])
         if lel > colwidth:
            colwidth = lel
            minwidths[c] = lel
 
            # But expanding this one may mean we can have fewer columns
            startcol = c
            if ncols > 1 and (sum(minwidths) + 2*len(minwidths)) > (width+2):
               return tabulate(lst, width, ncols-1)

   # If we got here, then the sizes fit!
   for r in range(0, nrows):
      line = []
      for c in range(0, ncols):
         i = c*nrows + r
         if i > (len(lst)-1):
            break
         line.append("%-*s" % (minwidths[c], lst[i]))
      line = string.join(line, '  ')
      line = line.rstrip()
      print line

def collapseIntervals(lst):
   """Take list of (start,end) interval tuples and return a similar list with all overlapping intervals combined."""
   intervals = []
   for (sstart,send) in lst:
      found = False
      for i in range(0, len(intervals)):
         (start,end) = intervals[i]
         if (sstart <= end and send > end) or (send >= start and sstart < start):
            # Expand interval (note this may make this interval overlap with one of the ones already in the list)
            start=min(start,sstart)
            end=max(end,send)
            #print >>sys.stderr, "Changing", intervals[i], "to", (start,end)
            intervals[i] = (start,end)
            found = True
            break
         elif send <= end and start >=start:
            # Already encompassed
            found = True
      if not found: 
         # Add new interval to list
         intervals.append((sstart,send))
         #print >>sys.stderr, "Adding", (sstart,send), intervals

   # Repeat until no more merges left
   while lst != intervals:
      lst = intervals
      intervals = collapseIntervals(lst)

   return intervals

if RegressionTest:
   assert(collapseIntervals([(1,3), (2,4), (2,3), (1,4), (0,2), (5,6), (4,4.5)]) == [(0,4.5), (5,6)])

def tallyStats(statlist, starttime, wallclock, nodecount):
      # Tally the stats
      errcodes = {}
      wtime = [0.0, 0.0]
      utime = [0.0, 0.0]
      stime = [0.0, 0.0]
      bytes = [0.0, 0.0]
      nodewtime = [{}, {}]
      nodebytes = [{}, {}] 
      allstats = []
      maxend = 0
      minstart = starttime

      for stats in statlist:
            ec = os.WEXITSTATUS(stats['status'])
            errcodes[ec] = errcodes.get(ec, 0) + 1

            if stats['timestamp'] < starttime:
               cached = 1
            else:
               cached = 0

            wtime[cached] += stats['time']
            utime[cached] += stats['utime']
            stime[cached] += stats['stime']
            bytes[cached] += stats['inputsize']

            minstart = min(minstart, stats['timestamp'] - stats['time'])
            maxend = max(maxend, stats['timestamp'])

            nodename = stats['nodename']
            nodewtime[cached][nodename] = nodewtime[cached].get(nodename, 0) + stats['time']
            nodebytes[cached][nodename] = nodebytes[cached].get(nodename, 0) + stats['inputsize']
           
      # Print out summary info
      errstr = ''
      for k in errcodes.keys():
         if not errstr: 
            errstr = "%d processes returned %d" % (errcodes[k], k)
         else:
            errstr += "; %d x %s" % (errcodes[k], k)
      if not errstr:
         errstr = 'No processes ran'

      print >>sys.stderr, errstr

      MB = 10**6

      if Verbose > 0:
         for n in nodebytes[0]:
            if nodewtime[0][n]:
               print >>sys.stderr, "Node %s %g MB/s new work" % (n, nodebytes[0][n]/MB/nodewtime[0][n])

      swtime = sum(wtime)

      if swtime:
         sbytes = sum(bytes)
         sutime = sum(utime)
         sstime = sum(stime)
         print >>sys.stderr, "Serial performance: %g MB/s, %.1fs (%.0f%% User, %.0f%% System, %.0f%% Wait)" \
            % (sbytes/MB/swtime, swtime, 100*sutime/swtime, 100*sstime/swtime, 100*(swtime-sstime-sutime)/swtime)

         if bytes[0] and wallclock:
            rate = bytes[0]/MB/wallclock
            scaling = (stime[0]+utime[0])/wallclock
            print >>sys.stderr, "New: %g MB/s, %.1f/%dx CPU scaling." % (rate, scaling, nodecount)
         
         # Find intervals we were computing over 
         intervals = [(s['timestamp'] - s['time'], s['timestamp']) for s in statlist]
         intervals = collapseIntervals(intervals)
         #print >>sys.stderr, intervals

         if bytes[1]:
            # Find intervals that were before starttime (cached)
            cacheduration = 0
            for (start,end) in intervals:
               if end < starttime:
                  cacheduration += (end-start)

            rate = bytes[1]/MB/cacheduration
            scaling = swtime/cacheduration
            print >>sys.stderr, "Cached: %g MB/s, %gs, %.1fx" % (rate, cacheduration, scaling)

         if True or sbytes and wallclock:
            rate = sbytes/MB/wallclock
            cached = 100*bytes[1]/sbytes
            print >>sys.stderr, "Filemap: %g MB/s, %.1fs, %.0f%% cached." % (rate, wallclock, cached)
  
def mkdirexist(path):
   try:
      os.makedirs(path)
   except OSError, e:
      if e.errno == errno.EEXIST:
         pass
      else:
         raise

def rsyncDeWildcard(pth):
   """Take a path, potentially including wildcards, and reduce to rsync args that can't expand."""
   pth = pth.replace('//', '/').replace('/./','/')

   origpath = pth

   # Reduce pth to a non-wildcard path
   while '*' in pth or '?' in pth:
      pth = os.path.dirname(pth)

   # Find the wildcard portion of the original path and make it a glob
   if len(pth) < len(origpath):
      includes = [origpath[len(pth):].lstrip("/")]
   else:
      includes = []

   # Include all parents of the wildcard pattern
   if includes:
      x = includes[0]
      while True:
         dir = os.path.dirname(x)
         if dir == x:
            break
         x = dir
         includes.append(x + "/")

   # Now include the _destination_ sub-path of the base path
   # For non-relative, this is just the basename
   includes.append(os.path.basename(pth))

   return (includes, [pth])

def rsyncSimplify(lst):
   includes = []
   paths = []
   for l in lst:
         [newi, newp] = rsyncDeWildcard(l)
         includes += newi
         paths += newp
      
   includes = ["--include=" + x for x in includes] 
   includes.append("--exclude=*")
   return [includes, paths]
  
def coallesce(outputs, labels):
   """Take a list of multi-line strings and an equally long list of labels and return a string summarizing which labels had what output."""
   assert(len(outputs) == len(labels))
   sameas = {}
   result = ""

   for i in range(0, len(outputs)):
      if outputs[i]:
         header = [labels[i]]
         for j in range(i+1, len(outputs)):
            if outputs[j] and outputs[i].getvalue() == outputs[j].getvalue():
               header.append(labels[j])
               outputs[j] = None

         header.sort(cmp=numcmp)
         header = string.join(header, ',')
         header += ": "
         
         for line in outputs[i]:
            result += header + line

   return result
 
def multiglob(globs):
   """Take a list of globs and return a list of all of the files that match any of those globs."""

   assert(type(globs) == type([]))
   ret = []
   for g in globs:
      ret += glob.glob(g)
   return ret


num_re = re.compile('^(([\D]+)|(\d+))')

def tokenize(x):
   xlist = []

   while x:
      xn = num_re.match(x)
      if not xn:
         return x
      xlen = len(xn.group(0))
      token = x[:xlen]
      try:
         token = int(token)
      except:
         pass
      xlist.append(token)
      x = x[xlen:]

   return xlist


def numcmp(x, y):
   """Determine order of x and y (like cmp()), but with special handling for 
      strings that have non-numeric and/or numeric substrings.  Numeric 
      substrings are ordered numerically even if they have differing numbers 
      of digits."""
   
   # Turn each input in to a tokenized list that we can cmp() natively
   xlist = tokenize(x)
   ylist = tokenize(y)
   r = cmp(xlist, ylist)
   return r

def statOverlap(joba, jobb):
   """Return true iff duration of joba does not overlap with duration of jobb."""

   if jobb['start'] > joba['finish'] or joba['start'] > jobb['finish']:
      return False
   return True

def makeVnodes(nodes):
   # We may have multiple jobs at a time per node (e.g. SMP nodes), so 
   # treat those as multiple nodes.  Since we don't have an explicity thread id, 
   # just assign them arbitrarily in a non-overlapping way
   for nodename in nodes.keys():
      vnodes = []
      for job in nodes[nodename]:
         found = False
         for vnode in vnodes:
            conflict = False
            for j in vnode:
               if statOverlap(job, j):
                  #print "Conflict", job, "\nwith", j
                  conflict = True
                  break
            if not conflict:
               # Does not overlap with any job in this vnode, so put it here
               vnode.append(job)
               found = True
               break
         if not found:
            # Make new node
            vnodes.append([job])
         #print "now", vnodes

      if len(vnodes) < 2:
         # It was serial, so no virtual nodes required
         continue

      del nodes[nodename]
      #print "Replacing", nodename, "with", len(vnodes)
      for i in range(0, len(vnodes)):
         nodes[nodename + "." + str(i)] = vnodes[i]
         #print nodes.keys()

   return nodes
   
def plot(filename, pngfile):
   import matplotlib.figure
   import matplotlib.collections
   import matplotlib.backends.backend_agg
   import matplotlib.pyplot as plt

   statsfile = open(filename, 'r')
   stats = cPickle.load(statsfile)
   statsfile.close()

   # Load all stats and keep track of min and max inputsizes along the way
   nodes = {}
   minsize = 2**31
   maxsize = 0
   for s in stats:
      s['start'] = s['timestamp'] - s['time']
      s['finish'] = s['timestamp']
      if not nodes.get(s['nodename']):
         nodes[s['nodename']] = []
      nodes[s['nodename']].append(s)

      minsize = min(minsize, s['inputsize'])
      maxsize = max(maxsize, s['inputsize'])

   nodes = makeVnodes(nodes)
         
   nodenames = nodes.keys()
   nodenames.sort(cmp=numcmp)

   # Build map of nodenames to y coordinates
   nodes2y = {}
   i = 0
   for n in nodenames:
      nodes2y[n] = i
      i += 1

   fig = matplotlib.figure.Figure(figsize=(8,10.5))
   ax = fig.add_subplot(111)
   ax.set_autoscale_on(True)
  
   #l = matplotlib.lines.Line2D([1, 0.8e9], [2,2])
   #ax.add_line(l)
   #l = matplotlib.lines.Line2D([1269190522.252409, 1269190522.2774091], [14, 14])
   #ax.add_line(l)

   lines = []
   maxsize = math.log(maxsize)
   if not minsize:
      minsize = 0
   else:
      minsize = math.log(minsize)

   for node in nodes:
      y = nodes2y[node]

      for s in nodes[node]:

         finish = s['finish']
         start = s['start']

         # Size is a log-scal enumber between 0 and 1 where 0 is the minimum size seen in this run and 1 is the max size seen in this run
         insize = s['inputsize']
         if insize:
            insize = math.log(insize)
         else:
            insize = 0
         size = (insize-minsize)/(maxsize-minsize)
   
         # CPU is % of wallclock time in user or system CPU time
         if s['time'] == 0:
            cpu = 1
         else:
            cpu = (s['utime'] + s['stime']) / s['time']
   
         l = matplotlib.lines.Line2D([start, finish], [y,y], marker='|', alpha=0.25+cpu, lw=1 + 2*size)
         ax.add_line(l)
         lines.append([ (start,y), (finish,y) ])

   ax.autoscale_view()
  
   #ax.set_yticks(range(0, i))
   #ax.set_yticklabels(nodenames)

   majorlabels = []
   majorlocs = []
   for n in nodenames:
      if n.endswith(".0"):
         majorlabels.append(n.replace(".0",""))
         majorlocs.append(nodes2y[n])
   #print >>sys.stderr, "Yticks", majorlocs, majorlabels
   ax.set_yticks(majorlocs)
   ax.set_yticklabels(majorlabels)
   ax.set_ylim(ymin=-2)

   canvas = matplotlib.backends.backend_agg.FigureCanvasAgg(fig)
   canvas.print_figure(pngfile, dpi=600)

class MethodOptionParser(optparse.OptionParser):
   """OptionParser to be instantiated by dispatch methods of a class.  Sets %prog to be the calling function name and epilog to be the doc string for that function.  Also prints options in usage string more like man conventions instead of just [options]."""

   def __init__(self, *args, **kwargs):
      if not kwargs.has_key('epilog'):
         caller = sys._getframe(1)
         pmethod = caller.f_code.co_name
         pself = caller.f_locals['self']
         method = pself.__class__.__dict__[pmethod]
         kwargs['prog'] = pmethod
         self.Epilog = method.__doc__ or ""

      if kwargs.has_key('fixed'):
         self.fixed = kwargs['fixed']
         del kwargs['fixed']
      else:
         self.fixed = None

      self.numrequired = 0
      if self.fixed:
         for i in self.fixed:
            if i[0] != "[":
               self.numrequired += 1 

      optparse.OptionParser.__init__(self, *args, **kwargs)
      self.disable_interspersed_args()

   def parse_args(self, args):
      (options, args) = optparse.OptionParser.parse_args(self, args)
      if len(args) < self.numrequired:
         print >>sys.stderr, "Required argument(s) missing"
         self.print_help()
         sys.exit(1)
      return (options, args)

   def get_usage(self):
      if self.fixed != None:
         flags = []
         options = ''
         for o in self.option_list:
            if o.action == 'store_true':
               flags += o._short_opts[0].lstrip('-')
            elif o.action == 'help':
               pass
            else:
               options += "[%s %s]" % (o._short_opts[0], o._long_opts[0].lstrip('-').upper())
               if o.action == 'append':
                  options += '...'
               
               options += ' '

         flags = string.join(flags, '')
         if flags:
            options += "[-" + flags + "] "
         
         self.usage = "%prog " + options + string.join(self.fixed)

      return optparse.OptionParser.get_usage(self)

   def print_help(self):
      r = optparse.OptionParser.print_help(self)
      
      # For python < 2.5, we have to print our own epilog 
      print
      print self.Epilog
      return r

class FmLocations:
   def __init__(self, config=None, locs={}, stdin=False):
      self.locs = locs # Allow pick() to derive an instance with a subset of the classes
      self.procs = Procs()
      self.cleanup = None

      if config:
         self.config = config

      else:
         self.config = ConfigParser.SafeConfigParser()

         # Get appropriate self.configstr and set FMCONFIG
         if stdin:
            # Read config file from stdin
            self.configstr = sys.stdin.read()

            # Keep a copy of the config around so our children can reference it
            fd, name = tempfile.mkstemp(prefix="tmp-fm-locations-")
            self.cleanup = name
            os.environ['FMCONFIG'] = name
            fh = os.fdopen(fd, 'w')
            fh.write(self.configstr)
            fh.close()

         else:
            filename = os.environ.get('FMCONFIG')
            #print >>sys.stderr, "Env filename is", filename
            if not filename:
               if os.path.isfile("filemap.conf"):
                  filename = "filemap.conf"
               else:
                  filename = "/etc/filemap.conf"

               os.environ['FMCONFIG'] = filename

            try:
               fh = open(filename)
            except:
               raise Exception("Unable to locate config file.  Set FMCONFIG or place filemap.conf in . or /etc")

            # Read into a buffer that we can relay to children 
            self.configstr = fh.read()

            #print >>sys.stderr, "Reading config", filename, self.config.sections()

         # Parse it
         filebuf = cStringIO.StringIO()
         filebuf.write(self.configstr)
         filebuf.seek(0)
         self.config.readfp(filebuf)

         self.replication = int(self.config_get("global", "replication", "1"))
         self.umask = int(self.config_get("global", "umask", "0002"))
         path = self.config_get("global", "pathextras")
         if path:
            os.environ['PATH'] += ":" + path
         os.umask(self.umask)

      # Unless we were passed an explicit list of locations, grab all
      # from the config file.
      if not locs:
         for s in self.config.sections():
            if s == "global": 
               continue
            l = self.parseLocation(s)
            self.locs[s] = l

      # Use global config values by default (thisis() can override later)
      self.this = self.parseLocation("global")

   def __del__(self):
      if self.cleanup:
         os.unlink(self.cleanup)

   def parseLocation(self, stanza):
      l = FmLocation()
      l.name = stanza

      qbyhost = self.config.has_option("global", "queuebyhost")

      l.sshcmdstr = os.path.expanduser(self.config_get(stanza, "ssh", "ssh -o GSSAPIDelegateCredentials=yes -o ConnectTimeout=5 -o StrictHostKeyChecking=no -Ax", raw=True))
      l.sshcmd = shlex.split(l.sshcmdstr, comments=True)

      l.rsynccmd = shlex.split(self.config_get(stanza, "rsync", "rsync -tO", raw=True), comments=True)
      l.rsynccmd[0] = os.path.expanduser(l.rsynccmd[0])

      if l.name != "global": 
         l.rootdir = os.path.expanduser(self.config_get(stanza, "rootdir"))
         l.hostname = self.config_get(stanza, "hostname")
         l.syncdir = os.path.expanduser(self.config_get(stanza, "syncdir", "/tmp/fmsync"))

         l.jobdir = l.rootdir + "/jobs"
         if qbyhost:
            l.qname = l.hostname
         else:
            l.qname = l.name

         pythoncmd = os.path.expanduser(self.config_get(stanza, "python", "python"))
         l.fmcmd = [pythoncmd, os.path.expanduser(self.config_get(stanza, "fm", l.rootdir + "/sys/fm"))]
         l.processes = int(self.config_get(stanza, "processes", "1000"))
         l.inactive = self.config_get(stanza, "inactive", False)
         l.cpusperjob = int(self.config_get(stanza, "cpusperjob", "1"))
         l.dynamicload = self.config_get(stanza, "dynamicload", True)

      return l
   
   def config_get(self, section, key, defval=None, raw=False):
      if self.config.has_option(section, key):
         return self.config.get(section, key, raw=raw)
      elif self.config.has_option("global", key):
         return self.config.get("global", key, raw=raw)
      else:
         return defval

   def thisis(self, nodename):
      self.thisname = nodename
      self.this = self.locs[nodename]

   def pickHosts(self):
      """Return a subset of locations containing only one location per hostname"""
      hosts = set()
      newlocs = {}
      for l in self.locs.values():
         if l.hostname in hosts:
            continue
         else:
            newlocs[l.name] = l
            hosts.add(l.hostname)
 
      newlocs = FmLocations(self.config, locs=newlocs)
      newlocs.this = self.this
      return newlocs
      
   def nodes(self, seed=None):
      """Generate a list of pseudo-random locations.  
      Specify a seed to get deterministic results (e.g. for a given extension 
      number, file name, path, etc).  Returns a generator of a list of node names."""

      candidates = self.locs.keys()

      if seed:
         random.seed(seed)

      while candidates:
         l = random.choice(candidates)
         candidates.remove(l)
         if not self.locs[l].inactive:
            yield l
 
   def pickn(self, seed=None, n=None):
      """Pick some pseudo-random locations.  The number of locations
      chosen is based on the replication factor in the config.
      Specify a seed to get deterministic results (e.g. for a given
      extension number.)  The return value is the list of node names."""

      if not n: 
         n = self.replication

      nodes = self.nodes(seed=seed)

      i = 0
      for l in nodes:
         i += 1
         if i > n:
            break
         yield l

      if i <= n:
         print >>sys.stderr, "Cannot pick %d locations" % (n)

   def pick(self, n=None, seed=None, selfIsNoop=False):
      """Return a new FmLocations instance that uses a subset of the
locations of this instance."""

      used = self.pickn(seed, n=n)

      if selfIsNoop and self.thisname in used: 
         used.remove(self.thisname)

      useddict = {}
      for u in used: 
         useddict[u] = self.locs[u]

      picked = FmLocations(self.config, locs=useddict)
      picked.this = self.this
      return picked
         
   def forAllLocal(self, cmd):
      """Same ase forAll(), but CMD will be run as arguments to "fm _local"."""
      if Verbose: 
         verbose = "-" + "v" * Verbose
         lcmd = ["fm", verbose, "_local", "-n", "%(NODE)"] + cmd
      else:
         lcmd = ["fm", "_local", "-n", "%(NODE)"] + cmd

      return self.forAll(lcmd, subst=True, stdinstr=self.configstr, tag=string.join(cmd))

   def forAll(self, Cmdv, src=[], dst=[], absolute=False, subst=False, trailer=None, glob=True, stdinstr=None, call=False, tag=None):
      if src and type(src) != type([]):
         src = [src]
      if dst and type(dst) != type([]):
         dst = [dst]

      if not tag:
         tag = string.join(Cmdv + src + dst)

      if stdinstr:
         stdin = subprocess.PIPE

      for loc in self.locs.values():
         if loc.inactive:
            continue

         if subst: 
            cmd = [n.replace("%(ROOTDIR)", loc.rootdir).replace("%(SYNCDIR)", loc.syncdir).replace("%(NODE)", loc.name) for n in Cmdv]
            src = [n.replace("%(SYNCDIR)", loc.syncdir) for n in src]
         else:
            cmd = list(Cmdv)

         if absolute:
            s = list(src)
            d = list(dst)
         else:
            s = [loc.rootdir + "/" + f for f in src]
            d = [loc.rootdir + "/" + f for f in dst]

         if cmd[0] == "fm":
            cmd = loc.fmcmd + cmd[1:]
         if isremote(loc.hostname):
            cmd = loc.sshcmd + [loc.hostname] + cmd + s + d

         else:
            if s and glob:
               #time.sleep(random.random())  #RHEL5 + thumper glob issues
               s = multiglob(s)
               if not s:
                  continue

            if d and glob:
               d = multiglob(d)
               if not d:
                  continue

            cmd += s + d 
            
         if trailer:
            cmd += trailer

         self.procs.Popen(cmd, call=call, stdinstr=stdinstr, queue=loc.qname, tag=tag)


      return self.procs
   
   def put(self, srclist, dst, relative=False, procs=None, pickn=False, hashbasename=False, all2all=False):
      origdst = dst
      if not procs:
         procs = Procs()

      if relative:
         srclist = [s.lstrip("/") for s in srclist]
         os.chdir(self.this.rootdir)

      srcs = multiglob(srclist)
      if not srcs:
         print >>sys.stderr, "No such file:", srclist
         return procs

      # We always specify a seed when calling pick() so there is stability in which nodes work is mapped to.
      # It's nice if they go to nodes that might have already received the store in a previous execution.

      items = []
      for a in srcs:
         # Convert directory arguments to lists of files
         if os.path.isdir(a):
            for root,dirs,files in os.walk(a):
               for f in files:
                  fp = os.path.join(root, f)
                  dstfile = os.path.join(dst, root[0:]) + "/" + os.path.basename(fp)
                  items.append([fp, dst, True])
         else:
            items.append([a, dst, relative])

      bydst = {}
      bydstrel = {}
      qname = {}
      for i in items:
         src, dst, relative = i
         if pickn:
            seed = os.path.basename(src)
            if not hashbasename:
               if relative:
                  seed = dst + "/" + src
               else:
                  seed = dst + "/" + seed
            choices = self.pickn(seed=seed, n=pickn)
            choices = [self.locs[c] for c in choices]
         else:
            choices = self.locs.values()

         for loc in choices:
            if loc.inactive: continue

            if isremote(loc.hostname):
               d = loc.hostname + ":" + loc.rootdir 
            else:
               d = loc.rootdir

            d = d + "/" + dst
            d = d.replace("//","/")
 
            if relative:
               if not bydstrel.get(d): bydstrel[d] = []
               bydstrel[d].append(src)
            else:
               if not bydst.get(d): bydst[d] = []
               bydst[d].append(src)

            qname[d] = loc.qname

      dsts = bydst.keys()
      random.shuffle(dsts)
      for dst in dsts:
         if all2all:
            queue = "all2all"
         else:
            queue = qname[dst]  

         cmd = self.this.rsynccmd + ["-Le", loc.sshcmdstr] + bydst[dst] + [dst]
         procs.Popen(cmd, queue=queue, node=qname[dst], tag="put %s %s" % (srclist, origdst))

      dsts = bydstrel.keys()
      random.shuffle(dsts)
      for dst in dsts:
         if all2all:
            queue = "all2all"
         else:
            queue = qname[dst]  

         cmd = self.this.rsynccmd + ["-Le", loc.sshcmdstr, "-R"] + bydstrel[dst] + [dst]
         procs.Popen(cmd, queue=queue, node=qname[dst], tag="put %s %s" % (srclist, origdst))

      return procs

   def get(self, args, procs=None, outfile=sys.stdout, relative=False):
      """Copy files from the cloud to local storage.

Note that if the source is wildcard, then output will be put in a 
sub-directory of the destination.  The name of that subdirectory 
will be the directory name preceding the first wildcard."""
      p = MethodOptionParser(fixed=["src... dst"])
      p.add_option("-c", "--cat", action="store_true", help="Cat files to stdout")
      p.add_option("-n", "--name", action="store_true", help="Show file name when catting files")
      (options, args) = p.parse_args(args)

      if (not options.cat) and (len(args) < 2):
         p.print_help()
         return 1

      rsyncflags = "-rptgo"
      if relative:
         rsyncflags += "R"

      # If relative=True, then ssh -R will build a destination tree for you
      # If relative=False and there is no wildcard, then the file will be put in dst
      # If relative=False and you use a wildcard, then you will get a tree built in dst
      # because we move globs to include/excludes and use --recursive
      # but if your wildcard matches more than one file and the last path component isn't a wildcard, 
      # then you probably want the tree or else files of the same basename would stomp on each other

      if options.cat:
         dst = tempfile.mkdtemp(prefix="tmp-fm-get-") + "/"
         src = args
      else:
         dst = args[-1]
         src = args[:-1]

      if not procs:
         procs = Procs()

      for loc in self.locs.values():
         if loc.inactive: continue

         # To make relative paths shorter in rsync >= 2.6.7 , put "/./" 
         # in the path between the rootdir and the relative path
         s = [loc.rootdir + "/./" + f for f in src]

         # Cleanup the path to encourage rsync to get "/./" magic right
         s = [i.replace('//','/') for i in s]

         #print loc.rootdir, src, s

         # Take the specified paths/globs and turn them into a list of simple (non-glob) paths and --include/exclude options
         [includes, s] = rsyncSimplify(s)

         if isremote(loc.hostname):
            s = [loc.hostname + ":" + string.join(s)]
         else:
            s = multiglob(s)
            if not s:
               #print >>sys.stderr, "No such file:", src
               continue

         cmd = loc.rsynccmd + includes
         cmd += [rsyncflags, "--prune-empty-dirs", "--copy-unsafe-links", "-e", loc.sshcmdstr] + s + [dst]

         procs.Popen(cmd, queue=loc.qname, tag="get %s" % (string.join(args)))

      procs.collect()

      if options.cat:
         empty = True
         for (dirpath, dirnames, filenames) in os.walk(dst):
            for f in filenames:
               fname = os.path.join(dirpath, f)
               if os.path.getsize(fname):
                  if options.name:
                     relname = fname[len(dst):]
                     print >>outfile, "=== %s ===" % relname
                     empty = False
                  outfile.writelines(file(fname))
         if options.name and not empty:
            print >>outfile, "======"

         shutil.rmtree(dst)

      return procs

   def processes(self):
      return sum(i.cpusperjob for i in self.locs.values())
         
class FmLocation:
   def __init__(self):
      pass

class Procs:
   """This class is used to launch some number of child processes and reap them.
   It provides methods to instantiate a process for each node."""

   def __init__(self, retries=3):
      self.queues = {}
      self.poll = select.poll()
      self.fd2proc = {}
      self.pids = {}
      self.numprocspolling = 0
      self.retries = retries

   def isempty(self):
      return (not self.pids)

   def Popen(self, cmdv, **kwargs):
      kwargs['cmdv'] = cmdv
      queue = kwargs.get('queue')
      if not kwargs.get('tag'):
         kwargs['tag'] = cmdv[0]+"..."
      if not kwargs.get('retries'):
         kwargs['retries'] = self.retries
      
      if queue and self.queues.get(queue) != None:
         # If queue exists (even if it is an empty queue), then something is running
         if Verbose > 2:
            print >>sys.stderr, "Queueing", kwargs['tag'], "to", queue, len(self.queues[queue])
         self.queues[queue].append(kwargs)
         return
      else:
         # No queue, means we can run now, but create an empty queue to block subsequent tasks         
         self.queues[queue] = []
         return self.PopenSave(**kwargs)

   def PopenSave(self, **kwargs):
      # Call PopenNow, but save a copy of the kwargs for later retries
      proc = self.PopenNow(**kwargs)
      proc.kwargs = kwargs
      return proc

   def PopenNow(self, cmdv=[], prefix=None, stderr=subprocess.PIPE, stdout=subprocess.PIPE, call=False, tag=None, queue=None, node=None, stdinstr=None, cwd=None, retries=0):
      # This function should only be called by PopenSave

      # Note, we prefer to have a stdout so that poll() can tell when it closes and then reap the child

      if stdinstr:
         stdin = subprocess.PIPE
      else:
         stdin = file('/dev/null','w')

      r = subprocess.Popen(cmdv, stdin=stdin, stdout=stdout, stderr=stderr, cwd=cwd)

      if r.stderr or r.stdout:
         self.numprocspolling += 1
         if r.stdout:
            fd = r.stdout.fileno()
            self.poll.register(r.stdout, select.POLLIN|select.POLLHUP|select.POLLERR) 
            self.fd2proc[fd] = r

            # make non-blocking file
            fl = fcntl.fcntl(fd, fcntl.F_GETFL)
            fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)

            r.stdoutbuf = cStringIO.StringIO()
   
         if r.stderr:
            fd = r.stderr.fileno()
            self.poll.register(r.stderr, select.POLLIN|select.POLLHUP|select.POLLERR) 
            self.fd2proc[fd] = r

            # make non-blocking file
            fl = fcntl.fcntl(fd, fcntl.F_GETFL)
            fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)

            r.stderrbuf = cStringIO.StringIO()

      r.tag = tag
      r.start = time.time()
      r.queue = queue

      if queue and not node:
         node = queue
      r.node = node

      self.pids[r.pid] = r
      if Verbose > 2:
         print >>sys.stderr, r.pid, cmdv, r.node

      if stdinstr:
         r.stdin.write(stdinstr)
         r.stdin.close()

      return r

   def wakeup(self, qname): 
         if not qname:
            return

         #Wake-up anything on this queue
         if self.queues[qname]:
            kwargs = self.queues[qname].pop(0)
            # Leave queue, even if empty, so things wait for us to finish
            if Verbose > 2:
               print >>sys.stderr, "Waking", kwargs
            self.PopenSave(**kwargs)
         else: 
            # Queues was already empty, so now delete it
            del self.queues[qname]

   def collect(self, ignoreErrors=False, labelPrint=False, maxErrors=None):
      """Execute any active or queued processes and return, a boolean success scalar and a list of the processes objects that ran."""
      results = []
      errors = 0
      total = 0
      while True:
         p = self.waitpid(True)
         if p == None:
            break

         results.append(p)
         total += 1
         if p.status:
            errors += 1 
         if Verbose > 1:
            ec = os.WEXITSTATUS(p.status)
            print >>sys.stderr, "%s: %gs status %d for %s" % (p.queue, p.time, ec, p.tag)
            waiting = [l.queue for l in self.pids.values()]
            waiting.sort()
            if waiting and len(waiting) < len(results): 
               print >>sys.stderr, "Still waiting on", string.join(waiting)

      if not results:
         return None, results

      tag = "for " + results[0].tag #XXX not al tags identical

      if Verbose > 0:
         print >>sys.stderr, "--- Report", tag
         tag = ""
         mean = 0.0
         for p in results:
            mean += p.time
        
         mean /= len(results)
         stddev = 0.0
         for p in results:
            stddev += (p.time - mean)**2

         stddev /= len(results)
         stddev = math.sqrt(stddev)

         if stddev > 0:
            for p in results:
               deviations = (p.time - mean) / stddev
               if deviations > 2:
                  print >>sys.stderr, "Node %s slow by %g standard deviations" % (p.node, deviations)

            print >>sys.stderr, "Average time %gs, stddev %gs %s" % (mean, stddev, tag)

      for k,v in self.queues.items():
         assert(not v)
      assert(not self.pids)
      assert(self.numprocspolling == 0)
      self.poll = select.poll() #Reinit, just in case not everything was unregister()'d
         
      if not ignoreErrors and errors:
         print >>sys.stderr, "Error, %d/%d subprocess(es) returned error %s" % (errors, total, tag)
            
      if labelPrint:
         sys.stdout.write(coallesce([f.stdout for f in results], [f.node for f in results]))
      if labelPrint or Verbose > 0:
         sys.stderr.write(coallesce([f.stderr for f in results], [f.node for f in results]))

      if Verbose > 0:
         print >>sys.stderr, "---"

      if not ignoreErrors and errors:
         if maxErrors != None and errors > maxErrors:
            sys.exit(errors)

      return errors, results

   def finalizeProc(self, p):
      # Record compute time
      after = os.times()
      p.time = time.time() - p.start
      p.utime = after[2] - p.starttimes[2]
      p.stime = after[3] - p.starttimes[3]
      del p.starttimes
      del p.start

      # Finish building the output buffers
      if p.stdout != None:
         # Make blocking again
         fd = p.stdout.fileno()
         fl = fcntl.fcntl(fd, fcntl.F_GETFL)
         fcntl.fcntl(fd, fcntl.F_SETFL, fl & (~os.O_NONBLOCK))

         self.poll.unregister(p.stdout)
         #print >>sys.stderr, "finalized", p.pid, p.stdout.fileno()
         del self.fd2proc[p.stdout.fileno()]
         p.stdoutbuf.write(p.stdout.read())
         p.stdout = p.stdoutbuf
         p.stdout.seek(0)

      if p.stderr != None:
         # Make blocking again
         fd = p.stderr.fileno()
         fl = fcntl.fcntl(fd, fcntl.F_GETFL)
         fcntl.fcntl(fd, fcntl.F_SETFL, fl & (~os.O_NONBLOCK))

         self.poll.unregister(p.stderr)
         del self.fd2proc[p.stderr.fileno()]
         p.stderrbuf.write(p.stderr.read())
         p.stderr = p.stderrbuf
         p.stderr.seek(0)

      if p.stderr or p.stdout:
         self.numprocspolling -= 1

      del self.pids[p.pid]

      # XXX: ssh returns 255 if it can't connect and rsync returns 12
      if p.status and p.kwargs['retries'] and (os.WEXITSTATUS(p.status) == 255 or os.WEXITSTATUS(p.status) == 12):
         # Requeue if retries enabled and child returned 255 (SSH failed to connect)
         p.kwargs['retries']  -= 1
         if Verbose > 0:
            print >>sys.stderr, p.kwargs['retries'], "more retries for", p.queue
         time.sleep(2**(self.retries - p.kwargs['retries'] - 4) + 0.5 * random.random())
         self.queues[p.queue].append(p.kwargs)
         self.wakeup(p.queue)
         return None
     
      self.wakeup(p.queue)

      return p


   def waitpid(self, block=False):
      """Check for any completed child, remove them from the procs dictionary and return them."""

      while not self.isempty():
         r = self.checkwait(block=block)
         if r:
            return r
         if not block:
            break

      return None

   def checkwait(self, block=False):
      if block: 
         # First, the block argument is just a hint, we're not obligated to block.
         # Second, we may have children that are blocking on I/O to us, so we have use poll().
         # But we may have children that have no shared fd with us and that can only be reaped with waitpid().
         # So we need to figure out which case it is to determine where we can actually block.

         if self.numprocspolling == len(self.pids):
            # In this case, waitpid() is superfluous, so don't block on it, but
            # block on poll()
            waitopts = os.WNOHANG
            pollopts = -1 # Block
         elif self.numprocspolling:
            # In this case we have to bounce back between waitpid() and poll()
            # We don't want to have a complete busy loop, so we give poll a small timeout.
            # This creates some potential latency, but this is not a expected case
            print >>sys.stderr, "Debug: inefficient when only some children have fds we manage"
            waitopts = os.WNOHANG
            pollopts = 2500 #block
         else:
            # In this case poll() is superfluous and we should just do a blocking waitpid()
            waitopts = 0 #block
            pollopts = 0
      else:
         # Simple case
         pollopts = 0
         waitopts = os.WNOHANG

      #print >>sys.stderr, "block case", waitopts, pollopts

      # Check for completed children.
      try:
         # Waiting register's child's CPU times, so measure right before and after
         starttimes = os.times()

         (pid, status) = os.waitpid(0, waitopts) 
         if pid:
            p = self.pids[pid]
            p.status = status
            p.starttimes = starttimes
            return self.finalizeProc(p)

      except OSError, e:
         if e.errno == errno.ECHILD:
            # If we're here its probably because in python 2.4, subprocess reaps other children
            # whenever making a new one.  So one of the children we thought was running had already
            # been waitpidded.  http://bugs.python.org/issue1731717
            # So, poll on each of our children to find the one to finalize:
            for p in self.pids.values():
               # First look for things reaped by the signal handler
               if p.__dict__.has_key('error'):
                  return self.finalizeProc(p)

               # Since the chils is already reaped, we'll probably measure no CPU time for it. oh well
               starttimes = os.times()
               status = p.poll()
               if status != None: 
                  p.status = status
                  return self.finalizeProc(p)    

            print >>sys.stderr, "Unexpected: no child", self.pids
            raise
         else:
            raise

      # Look for children that may be blocking on writes to stdout, or that have finished
      # XXX: this won't catch children that we don't have share a fd with
      try:
         # Make sure we get interrupted if a child exits (in case we're not connected to any of its fds)
         l = self.poll.poll(pollopts)

      except select.error, e:
         if e[0] == errno.EINTR:
            return None
         else:
            raise
      else:
         for fd,event in l:
            p = self.fd2proc[fd]

            if event & select.POLLIN: 
               if fd == p.stdout.fileno():
                  s = p.stdout.read()
                  p.stdoutbuf.write(s)
               else:
                  assert(fd == p.stderr.fileno())
                  s = p.stderr.read()
                  p.stderrbuf.write(s)

      return None

# The design is as follows:
#   1. Jobs are submitted in an uncoordinated manner, so they have unique
#      identifiers that aren't generated by the user. but that are identical
#      across all nodes running a job. 
#   2. To manage the problem of listing jobs and removing jobs, we use a 
#      directory structure to contain all current jobs.  Deleting a file should
#      (eventually) lead to a job not running further.
#   3. New jobs should be invoked synchrously by calling a job scheduler with a
#      hint that points to the job.  Failure to send this hint should only delay
#      the job scheduler discovering the job since it should periodically poll
#      for changes in the job directory.

class JobScheduler:
   """JobScheduler maintains a set of child processes working on a set of jobs."""

   def __init__(self, options):
      self.jobs = {}  # A dictionary of job objects indexed by job name
      self.procs = Procs(retries=0)
      self.options = options

      self.freethreads = self.options.processes

      if options.dynamicload:
         global cpu
         cpu = CpuProfile()

   def ReadJobDir(self):
      """Check for new/removed jobs"""

      jobs = os.listdir(self.options.jobdir)
      removed = set(self.jobs) - set(jobs)
      recent = set(jobs) - set(self.jobs)
      for j in removed:
         #print "Job", j, "removed"
         del self.jobs[j]

      for j in recent:
         #print "Job", j, "added"
         jobuid = os.stat(self.options.jobdir + "/" + j).st_uid
         if jobuid != os.getuid():
            #print >>sys.stderr, "Skipping job owned by UID %d", jobuid
            continue
         try:
            self.jobs[j] = FmJob(self.options.jobdir + "/" + j, cpusPerJob=self.options.cpusperjob)
         except:
            traceback.print_exc()
            print >>sys.stderr, "Error parsing job description", j, "; skipping"

   def RunOnce(self):
         """Try to launch a work item and return True on success (None->nothing to do)"""

         self.freethreads -= 1

         # Look for something with work to do
         jobs = self.jobs.keys()
         random.shuffle(jobs)
         for jobname in jobs:
            j = self.jobs[jobname]

            if self.options.dynamicload:
               if not cpu.available():
                  continue
            else: 
               if j.running >= j.threads:
                  continue

            proc = j.compute(self.options, self.procs)
            #print >>sys.stderr, "Launched", proc
            if proc: 
               proc.jobname = jobname
               return True
            else:
               # No more work to do, check to see if job is complete
               # Note, job cannot be complete if parent is still active
               if not j.continuous and not j.running and (not j.parent or not self.jobs.has_key(j.parent)):
                  self.JobDone(jobname)

         self.freethreads += 1
         return None

   def JobDone(self, jobname):
      """A job has run to completion, remove it locally."""

      del self.jobs[jobname]
      #print >>sys.stderr, "Job", jobname, "done"
      os.unlink(self.options.jobdir + "/" + jobname)

   def RunUntilDone(self):
      """Run until there is no more input on it.  Return True iff we did something"""

      didSomething = False
      sleep = 0.001

      while True:
         if self.freethreads:
            self.ReadJobDir()
            if self.procs.isempty() and not len(self.jobs):
               # Doing nothing and no more jobs to run
               return didSomething
            if self.RunOnce():
               didSomething = True
               block=False
            else:
               # Nothing to do right now
               # Don't return because we're still working,
               # but wait before we look for more work to do.
               #print >>sys.stderr, "Wait for completion (and/or more work)"

               # Sleeping hurts latency for detecting new inputs, but avoids busy waits.
               # So we do an exponential backoff in how long we sleep with a max of 1 second
               sleep *= 10
               if sleep > 1:
                  sleep = 1  # Max out at 1 sec sleep
               block=False # Don't block since new work could arrive before a child finishes
         else:
            block=True
            assert (not self.procs.isempty())

         p = self.procs.waitpid(block=block)
         #print >>sys.stderr, "WAITPID", block, p
         if p:
            self.freethreads += 1
            self.finalizeItem(p)
         else:
            if not block:
               time.sleep(sleep)

   def finalizeItem(self, p):
      stats = {}
      stats['status'] = p.status
      stats['time'] = p.time
      stats['utime'] = p.utime
      stats['stime'] = p.stime
      stats['inputsize'] = p.inputsize
      stats['jobname'] = p.jobname
      stats['nodename'] = self.options.name
      stats['hostname'] = socket.gethostname()
      #stats['schedpid'] = os.getpid()
      stats['timestamp'] = time.time()

      f = file(p.outdirname + "/." + p.outbasename + "/.status", "w")
      cPickle.dump(stats, f)
      f.close()
   
      dst = p.outdirname + "/" + p.outbasename 
      if os.path.exists(dst): 
          shutil.rmtree(dst)
          # By removing this dst, we're invalidating any previously derived data, but our redo-if-newer-than logic will handle the compute.

      print >> sys.stderr, "about to remove", p.outdirname , "/." , p.outbasename, "on pid", os.getpid()
      os.rename(p.outdirname + "/." + p.outbasename, dst)
      p.job.running -= 1 
     
class CpuProfile:
   def __init__(self):
      self.user = 0
      self.idle = 0
      self.iowait = 0
      self.update()
 
   def update(self):
      fields = open('/proc/stat').readline().split()
      assert(fields[0] == "cpu")
      usercpu = int(fields[1])
      idlecpu = int(fields[4])
      ioticks = int(fields[5])
      
      self.delta_user = usercpu - self.user
      self.delta_idle = idlecpu - self.idle
      self.delta_io = ioticks - self.iowait

      self.user = usercpu
      self.idle = idlecpu
      self.iowait = ioticks

      if Verbose > 1:
         print >>sys.stderr, "CPU user", self.delta_user, "idle", self.delta_idle, "I/O", self.delta_io
 
   def available(self):
      """Return True iff this job has been I/O bound."""
      self.update()

      if (self.delta_idle > 0) and (self.delta_io < self.delta_user):
         return True
      else:
         return False
      

class FmJob:
   """Each FmJob object represents a job, as specified in a job file, and
    provides methods for identifying and processing inputs for that job."""

   def __init__(self, fname, cpusPerJob = 1):
      config = ConfigParser.SafeConfigParser()

      processed = config.read(fname)
      if not processed:  
         raise IOError(errno.ENOENT, "Could not open file", fname)

      self.jobname = os.path.basename(fname)
      self.cmd = config.get("mrjob", "cmd", raw=True)
      try:
         self.cmd = eval(self.cmd)
      except:
         self.cmd = self.cmd.split()
      self.inputs = config.get("mrjob", "inputs")
      self.cachedinputs = []
      self.continuous = config.has_option("mrjob", "continuous")
      self.reduce = config.has_option("mrjob", "reduce")

      if config.has_option("mrjob", "procspercpu"):
         self.threads = int(config.get("mrjob", "procspercpu"))
      else:
         self.threads = 1
      self.threads *= cpusPerJob

      self.running = 0

      if config.has_option("mrjob", "parent"):
         self.parent = config.get("mrjob", "parent")
      else:
         self.parent = None

   def compute(self, options, procs):
      # First time thru we expand the input globs to a file list
      # Unless this is a continuous job, this will be the only time we enumerate the files
      if not self.cachedinputs:
         self.cachedinputs = multiglob([options.rootdir + "/" + x for x in self.inputs.split()])
         random.shuffle(self.cachedinputs)

      if self.reduce:
         parts = [os.path.basename(i) for i in self.cachedinputs]
         parts = list(set(parts))  # Get unique partitions

         for part in parts:
            # Don't process .d directories as named inputs (feedback loop)
            if part[-2:] == ".d":
               continue

            # Apply all of the files with the same partition,
            # but only if that partition belongs on this node.
            choices = Locations.pickn(part, n=1)
            if Locations.thisname not in choices:
               #print >>sys.stderr, Locations.thisname, "hashes", part, "to", list(choices)
               continue

            #Get list of files with this partition
            xlen = len(part)
            files = []
            for i in self.cachedinputs:
               if os.path.basename(i) == part:
                  files.append(i)
            self.cachedinputs = []

            outfilename = "/reduce/" + self.jobname + "/" + part

            p = self.computeItem(files, outfilename, options, procs)
            if p: 
               #print >>sys.stderr, "Reduce part", part, "on node", Locations.thisname, files, "of", self.inputs

               return p

         return None # Nothing to do

      while self.cachedinputs:
         i = self.cachedinputs.pop()

         if i[-2:] == ".d": 
            # Don't process .d directories as named inputs (feedback loop)
            continue

         relativename = i[len(options.rootdir):]
         (relativepath, relativebase) = os.path.split(relativename)

         filenamehash = sha(relativebase).hexdigest()
         topdir = filenamehash[:2]
         secdir = filenamehash[2:4]

         outfilename = relativepath + "/" + topdir + "/" + secdir + "/" + relativebase + ".d/" + escape(self.cmd)
        
         p = self.computeItem(i, outfilename, options, procs)
         if p:
            return p

      return None # Nothing to do

   def atomicLink(self, lockfilename):
      myname = lockfilename + "-" + socket.gethostname() + "-" + str(os.getpid())
      mkdirexist(os.path.dirname(lockfilename))

      syncfile = os.fdopen(os.open(myname, os.O_CREAT|os.O_SYNC|os.O_WRONLY), "w")
      syncfile.write(myname)
      syncfile.close()

      success = False

      try:
         os.link(myname, lockfilename)
         success = True
      except OSError, e:
         if e.errno != errno.EEXIST:
            os.unlink(myname)
            raise

      # Get rid of private name for it, successful or not
      os.unlink(myname)

      return success
    
   def computeItem(self, inputs, outfilename, options, procs):
      """Start (but don't wait for completion) running this job on of the next unprocessed input for this job."""
      if type(inputs) != type([]):
         inputs = [inputs]

      obase = options.rootdir + "/" + outfilename
      statfile = obase + "/" + ".status"

      # We invalidate outputs if inputs are newer, so find the newest input
      newestinput = 0
      for i in inputs:
         newestinput = max(newestinput, os.path.getmtime(i))

      # First check locally to see if output exists and is current
      if os.path.exists(statfile):
         lasttime = os.path.getmtime(statfile)
         if newestinput <= lasttime:
            #print >>sys.stderr, "Local results current for", statfile
            return None
         #print >>sys.stderr, "Old local results", statfile, lasttime, newestinput
      else: 
         #print >>sys.stderr, "No local results", statfile
         pass

      # If we get here, then we're locally prepared to rerun
      # Check global syncronization to see if another node already ran it
      syncfilename = options.syncdir + "/" + outfilename + "/status"
      if not self.atomicLink(syncfilename):
            # This is a common case; somebody has already grabbed
            # the synchronization file, so we don't need to process
            # this file --- unless the inputs are newer than the global outputs
            lasttime = os.path.getmtime(syncfilename)
            if newestinput > lasttime:
               #print >>sys.stderr, "Old global results", syncfilename, lasttime, newestinput

               # Invalidate (nuke) the syncfile
               try:
                  offname = syncfilename + "-" + socket.gethostname() + "-" + str(os.getpid())
                  os.rename(syncfilename, offname)
                  os.unlink(offname)

                  if not self.atomicLink(syncfilename):
                     # Somebody else won
                     return None
                     
               except:
                  # Somebody else was doing this at the same time, let them have it
                  return None
            else:
               # Didn't get lock, don't want to steal it, give up
               return None

      #print >>sys.stderr, "Got lock", syncfilename

      # Now we hold the "lock" on the syncfile, so proceed...

      # Now insert a "." at the beginning of the basename for partial output
      odirname = os.path.dirname(obase)
      obasename = os.path.basename(obase)
      obase = odirname + "/." + obasename

      oname = obase + "/stdout"
      ename = obase + "/.stderr"
      mkdirexist(obase)
      try:
         os.unlink(obase + "/.status")
      except:
         pass

      sout = os.fdopen(os.open(oname, os.O_CREAT|os.O_WRONLY|os.O_TRUNC), "w")
      serr = os.fdopen(os.open(ename, os.O_CREAT|os.O_WRONLY|os.O_TRUNC), "w")

      # Make a local copy of cmd so we can substitute this input file in it
      cmd = copy.copy(self.cmd)

      if '%(input)' in cmd:
         idx = cmd.index('%(input)')
         cmd[idx:idx+1] = inputs
      else:
         cmd += inputs
    
      if cmd[0] == "fm":
         cmd = options.fmcmd + cmd[1:]

      cmd = [os.path.expanduser(c) for c in cmd]
 
      try:
         p = procs.Popen(cmd, stdout=sout, stderr=serr, cwd=obase)
      except:
         #print >>serr, "Error", sys.exc_value, "executing", cmd
         print >>sys.stderr, "Error", sys.exc_value, "executing", cmd
         raise

      assert(p)
      #print >>sys.stderr, "JOB RUNNING", self.jobname, p, procs.pids
      p.inputsize = 0
      for i in inputs:
         p.inputsize += os.path.getsize(i)
      p.outdirname = odirname
      p.outbasename = obasename
      p.job = self
      self.running += 1

      sout.close()
      serr.close()

      #print >>sys.stderr, "computeItem", cmd, socket.gethostname(), p.pid
      return p

class CommandSetBase(object):
   """This is a base-class for defining methods which are used as
   command line-based commands.  You should inherit from it and define
   methods.  All methods will be callable.  Usage and help information
   will only include methods that do not begin with an underscore."""

   def __method(self, mname):
      return self.__class__.__dict__[mname]

   def _usage(self):
      """Return a multi-line usage string based on the defined methods and
their doc strings."""

      usage = 'For more help, specify one of the following commands followed by -h option:\n'
      keys = self.__class__.__dict__.keys()
      keys.sort()
      for cmd in keys:
         if cmd[0] == "_":
            continue
         method = self.__method(cmd)
         #docstr = pydoc.text.indent(pydoc.text.document(method)).replace("(self, args)","")
         docstr = method.__doc__
         if docstr:
            docstr = docstr.split("\n")[0].strip().rstrip(".")
         usage += "   %-8s - %s\n" % (cmd, docstr)
      return usage

   def __init__(self, args, optParser=None):
      if not optParser:  
         optParser = MethodOptionParser()
         optParser.set_usage("""%prog command args...\n""" + self._usage())
      (options, args) = optParser.parse_args(args)
   
      if not args: 
         optParser.print_help()
         sys.exit(1)

      try:
         method = self.__method(args[0])

      except:
         print >>sys.stderr, "Unknown command", args[0]
         optParser.print_help()
         sys.exit(1)

      sys.exit(method(self, args[1:]))

   def _optionHook(self, optParser):
      pass

class FmCommands(CommandSetBase):
   def __init__(self, args):
      """FileMap is a file-based map-reduce system.  
You can think of it is parallel and distributed xargs or make.  
It features replicated storage and intermediate result caching.

http://mfisk.github.com/filemap
"""

      self._locs = None
      p = MethodOptionParser()
      p.set_usage("""fm command args...\n""" + self._usage())
      p.add_option("-v", "--verbose", action="count", help="Increase verbosity (can be used multiple times)")
      (options, args) = p.parse_args(args)

      if options.verbose:
         global Verbose
         Verbose += options.verbose

      CommandSetBase.__init__(self, args, p)

   def _Locations(self):
      if not self._locs: 
         self._locs = FmLocations()
      return self._locs

   def split(self, args):
      """Partition an input file.
      Split an inputfile into n pieces.  By default, the first
      whitespace-delimited field is used as the key.  All lines with the
      same key will be placed in the same output file.  The -r option can be
      used to specify a Perl-compatible regular expression that matches the
      key.  If the regex contains a group, then what matches the group is
      the key; otherwise, the whole match is the key.

      Output files are numbered 1 to n and created in the current directory.
      """
      p = MethodOptionParser(fixed=["infile"])
      p.add_option("-n", "--nways", help="Number of output files to use")
      p.add_option("-r", "--regex", help="Regex that matches the key portion of input lines")
      p.add_option("-f", "--field", default=1, help="Use field # (starting with 1).")
      (options, args) = p.parse_args(args)
      options.nways = int(options.nways)

      infile = args[0]

      #print >>sys.stderr, "Writing to", ofile

      if not options.nways:
         p.error("-n option required")
     
      if options.regex:
         options.regex = re.compile(options.regex)
 
      if options.field:
         options.field = int(options.field) - 1

      files = []
      for i in range(0, options.nways):
         fname = str(i+1)
         files.append(file(fname, "w"))

      for line in file(infile):
         if options.regex:
            key = options.regex.search(line)
            if key:
               g = key.groups()
               if len(g):
                  key = g[0]
               else:
                  key = key.group(0)
            else:
               print >>sys.stderr, "Key not found in line:", line.rstrip()
         else:
            words= line.split()
            if options.field >= len(words):
               key = None
            else:
               #print words[options.field], words
               key = words[options.field]

         i = hash(key) % options.nways
         files[i].write(line)

      for f in files: f.close()

   def kill(self, args):
      """Kill a job."""
      (options, args) = MethodOptionParser(fixed=["jobid"]).parse_args(args)
      args = ["/jobs/" + a for a in args]
      self._Locations().forAll(["rm", "-f"], args).collect()

   def mv(self, args):
      """Rename files in the cloud"""
      (options, args) = MethodOptionParser(fixed=["src...", "dst"]).parse_args(args)
      dst = args[-1]
      each = args[:-1]
      if len(each) > 1: dst += "/"
 
      self._Locations().forAll(["mv"], each, dst).collect()

   def mkdir(self, args, absolutes=[], async=False):
      """Make a directory (or directories) on all nodes.  
Has unix "mkdir -p" semantics."""
      (options, args) = MethodOptionParser(fixed=["dir..."]).parse_args(args)
      # Make sure destination exists
      p = self._Locations().forAll(["mkdir", "-p", "-m", "2775"] + absolutes, args, subst=True, glob=False)
      if not async:
         p.collect()

   def jobs(self, args):
      """Show all of the jobs still active."""
      (options, args) = MethodOptionParser(fixed=[]).parse_args(args)

      pkls = self._local_pickles(["localjobs"])
      jobs = {}
      for mx in pkls:
         for jname,j in pkls[mx].iteritems():
            if type(j.cmd) == type([]):
               j.cmd = string.join(j.cmd)
            if not jobs.get(jname):
               jobs[jname] = j
               j.nodes = [mx]
            else:
               jobs[jname].nodes.append(mx)

      fmt = "%-28s %-9s %-15s\n                             %s\n                             %s"
      print fmt % ("Job ID", "User", "Nodes", "Command", "Inputs")
       
      for jname,j in jobs.iteritems():
         j.nodes.sort(cmp=numcmp)
         print fmt % (jname, j.uid, string.join(j.nodes, ','), j.cmd, j.inputs)

   def df(self, args):
      """Show free space on each node."""
      (options, args) = MethodOptionParser(fixed=[]).parse_args(args)

      self._Locations().forAll(["df", "-h"], ["/"]).collect(labelPrint=True)

   def chmod(self, args, absolutes=[], ignoreErrors=False):
      """Change permissions on files in the cloud."""
      (options, args) = MethodOptionParser(fixed=["mode", "files..."]).parse_args(args)
      self._Locations().forAll(["chmod", args[0]] + absolutes, args[1:], subst=True).collect(ignoreErrors=ignoreErrors)

   def chgrp(self, args, absolutes=[]):
      """Change GID on files in the cloud."""
      (options, args) = MethodOptionParser(fixed=["perm", "files..."]).parse_args(args)
      self._Locations().forAll(["chgrp", args[0]] + absolutes, args[1:], subst=True).collect()

   def store(self, args):
      """Store one or more files into the cloud."""

      """src... dst

Copy the specified file(s) into the virtual store.  
"""
      (options, args) = MethodOptionParser(fixed=["files...", "dst"]).parse_args(args)

      dst = args[-1]
      args = args[:-1]
      if len(args) > 1:
         dst += "/"

      self._Locations().put(args, dst, pickn=True).collect()

   def map(self, args):
      """Launch a computation on a set of input files in the cloud.
Run the specified command on each input file (in the virtual store)
described by the inputglob.  Multiple inputglob arguments can be
given.  The -c option says that the commond should continue to run on
new inputs as they arrive.

Multiple commands can be chained together with |.  Each output file of
the first command becomes an input for the next command in the
pipeline.

The -f option says that any previously cached output should be ignored
and the program re-run.
"""

      p = MethodOptionParser(fixed=["cmd [| cmd...]"])
      p.add_option("-i", "--inputglob", action="append", help="Glob of input files (in root)")
      p.add_option("-c", "--continuous", action="store_true", help="Continue to look for new input files to arrive")
      p.add_option("-f", "--fresh", action="store_true", help="Do not use any cached output")
      p.add_option("-q", "--quiet", action="count", help="Do not fetch and display stdout (or stderr if -qq)")
      p.add_option("-o", "--optimistic", action="store_true", help="Try to run even if there are many failures submitting job")
      p.add_option("-p", "--procspercpu", action="store", help="Number of processes to run per CPU (default=1)", default=1)
      p.add_option("-s", "--statusonly", action="store_true", help="Don't wait for completion; fetch any available results now")
      (options, cmd) = p.parse_args(args)

      if not options.inputglob:
         p.error("-i or --inputglob must be specified")

      fresh = options.fresh # options.fresh gets rewritten later

      cmdwords = shlex.split(string.join(cmd))
      cmds = []
      while True:
         if "|>" in cmdwords:
            idx = cmdwords.index("|>")
            cmdwords.insert(idx+1, ">")
            cmdwords[idx] = "|"
            continue
        
         if "|" in cmdwords:
            idx = cmdwords.index("|")
            cmd = cmdwords[:idx]
            cmds.append(cmd)
            assert(cmd)
            cmdwords = cmdwords[idx+1:]
         else:
            if not cmdwords:
               # We should probably warn the user they had an empty pipeline segment
               print >>sys.stderr, "Empty pipeline element in ", cmds
               break
            cmds.append(cmdwords)
            break

      inglobs = options.inputglob
      outdirglobs = []
      statglobs = []
      assert (type(inglobs) == type([]))

      tmpdir = tempfile.mkdtemp(prefix="tmp-fm-map-") + "/"

      starttime = None
      jobname = None

      if options.optimistic:
         maxErrors = None
      else:
         maxErrors = self._Locations().replication

      for cmd in cmds:
         reduce = False

         if cmd[0] == ">":
            cmd = cmd[1:]

            if not options.statusonly:
               # Install the jobs on each node
               self._Locations().put([tmpdir + "*"], "/jobs/").collect(maxErrors=maxErrors)

            shutil.rmtree(tmpdir)
            tmpdir = tempfile.mkdtemp(prefix="tmp-fm-map-") + "/"

            if not starttime:
               starttime = time.time()
            if not options.statusonly:
               # Barrier; Wait for that job (and its parents) to finish:
               if Verbose > 0:
                  print >>sys.stderr, "Waiting for map and redistribute.  Job", jobname
               self._Locations().forAllLocal(["wait", jobname, "redistribute"] + inglobs).collect(maxErrors=maxErrors)
      
            reduce = True

         jobname, inglobs = self._MapComponent(cmd, inglobs, outdirglobs, options, reduce=reduce, parent=jobname, tmpdir=tmpdir, procsPerCpu = int(options.procspercpu))

      if not options.statusonly:
         # Install the jobs on each node
         self._Locations().put([tmpdir + "*"], "/jobs/").collect(maxErrors=maxErrors)

      shutil.rmtree(tmpdir)
      if not starttime:
         starttime = time.time()

      if not options.statusonly: 
         if Verbose > 0:
            print >>sys.stderr, "Waiting for completion.  Job", jobname
         self._Locations().forAllLocal(["wait", jobname]).collect(maxErrors=maxErrors)

      wallclock = time.time() - starttime

      # Get the output files
      errglobs = [f + "/.stderr" for f in outdirglobs]
      statglobs += [f + "/.status" for f in outdirglobs]

      statlists = self._local_pickles(["localstats"] + statglobs)
      allstats = []
      for lst in statlists.values():
         allstats.extend(lst)

      nodecount = self._Locations().processes()
      tally = tallyStats(allstats, starttime, wallclock, nodecount)
      statsfile = open('.fm.stats', 'w')
      cPickle.dump(allstats, statsfile)
      statsfile.close()

      # Fetch stdout/stderr               
      if (not options.statusonly) and (options.quiet < 2):
         if options.quiet:
            outputglobs = []
         else:
            outputglobs = inglobs
         loc = tempfile.mkdtemp(prefix="tmp-fm-status-")
         self.get(outputglobs + errglobs + [loc], relative=True)

         for (dirpath, dirnames, filenames) in os.walk(loc):
            for f in filenames:
               fname = os.path.join(dirpath, f)
               fh = file(fname)

               if f == ".stderr":
                  # Do nothing if empty
                  if os.path.getsize(fname):
                     print >>sys.stderr, '===', fname, '==='
                     sys.stderr.writelines(fh)
                     print >>sys.stderr, '====='
               else:
                  sys.stdout.writelines(fh)
               
         shutil.rmtree(loc)

 
   def get(self, *args, **kwargs):
      """Copy one or more files from the cloud to local storage."""
      self._Locations().get(*args, **kwargs)

   def replicate(self, args):
      """Replicate the specified files (in the virtual file system) from each node."""
      (options, args) = MethodOptionParser(fixed=["glob..."]).parse_args(args)
  
      self._Locations().forAllLocal(["replicate"] + args).collect(labelPrint=True)

   def run(self, args):
      """Run a command on each node and print results."""
      (options, args) = MethodOptionParser(fixed=["cmd..."]).parse_args(args)
  
      self._Locations().forAll(args, subst=True).collect(labelPrint=True)

   def runhost(self, args):
      """Run a command on each unique host and print results."""
      (options, args) = MethodOptionParser(fixed=["cmd..."]).parse_args(args)
       

      self._Locations().pickHosts().forAll(args).collect(labelPrint=True)

   def cat(self, args):
      """Concatenate one or more files to stdout."""
      (options, args) = MethodOptionParser(fixed=["files..."]).parse_args(args)

      #self.get(["-c"] + args)
      procs = self._Locations().forAll(["cat"], args)
      (pcount, rprocs) = procs.collect(ignoreErrors=True)

      for f in rprocs:
         sys.stdout.writelines(f.stdout)

   def plot(self, args):
      """Plot run-time of a previous map."""
      (options, args) = MethodOptionParser(fixed=["[infile [outfile]]"]).parse_args(args)
      infile = ".fm.stats"
      pngfile = "fmstats.png"
      if args:
         infile = args[0]
      else:
         print >>sys.stderr, "Using %s as default input file" % infile
   
      if len(args) > 1:
         pngfile = args[1]
      else:
         print >>sys.stderr, "Using %s as default output file" % pngfile
   
      if len(args) > 2:
         print >>sys.stderr, "Usage: fm plot [<infile> [<outfile>]]"
         sys.exit(-1)
   
      plot(infile, pngfile)

   def _local(self, args):
      """This is for slave (remote) instantiations."""
      return FmLocalCommands(args)

   def _local_pickles(self, args):
      """Execute a command on each node and return a dict of the unpickled output from each node (node->output)."""

      procs = self._Locations().forAllLocal(args)
      status, ps = procs.collect()

      pickles = {}
      
      for p in ps:
         pkl = p.stdout.read()
         p.stdout.close()
         if not pkl: 
            continue

         try:
            unpkl = cPickle.loads(pkl)
         except:
            print >>sys.stderr, "Error", sys.exc_value, "parsing remote pickle results:", pkl
            return 1   # TODO: this value isn't ever expected/handled by the code calling this function
                        # we should return an empty list or check the list contents or at least the
                        # return type after this function is called
         pickles[p.queue] = unpkl

      return pickles

   def _ls(self, args):
      """generic way to get info about files in a blob"""             
      if not args: 
         args = ["/"]
      pkls = self._local_pickles(["localls"] + args)

      listing = {}

      for nodename in pkls:
         for f in pkls[nodename]:
            fname = f['name']
            f['nodename'] = nodename
            if not listing.has_key(fname): 
               listing[fname] = []
            listing[fname] += [f]

      return listing

   def fs(self, args):
      p = MethodOptionParser(fixed=["<dir>..."])
      p.add_option('-r', '--replication', action='store', help="Replication level to check for", default=self._Locations().replication)
      p.add_option('-f', '--fix', action='store_true', help="Fix file system")
      p.add_option('-v', '--verbose', action="count", help="Up verbostiy, more occurrances means higher level")
      p.add_option('-d', '--dryrun', action='store_true', help="Display proposed fix, do not execute")
      (options, args) = p.parse_args(args)

      options.replication = int(options.replication)

      listing = self._ls(args)

      toofew = set()
      toomany = set()
      total = len(listing.keys())

      for i in listing.keys():
         if len(listing[i]) < options.replication:
            toofew.add(i)
         if len(listing[i]) > options.replication:
            toomany.add(i)

      print total, "files found"
      print len(toofew), "files don't have enough replication"
      print len(toomany), "files have too much replication"
      print total - len(toofew) - len(toomany), "files' replication is just right"

      if options.dryrun:
         print "Need to up replication on:"
         print toofew
         print "Need to prune replication on:"
         print toomany
         sys.exit(0)

      if options.fix:
         pass
         # for i in too few, find differnece in replication and occurrences, select hosts to store
         # for i in too many, find differnece in replication and occurrences, select hosts to remove


   def ls(self, args):
      """File & directory listing."""
      p = MethodOptionParser(fixed=["[files...]"])
      p.add_option('-n', '--nodes', action='store_true', help="List names of nodes holding each file")
      p.add_option('-l', '--long', action='store_true', help="Show permissions, nodes, sizes, timestamps, etc.")
      p.add_option('-q', '--decode', action='store_true', help="Replace escaped characters")
      (options, args) = p.parse_args(args)

      ls = {}
      listing = self._ls(args)

      for fkey in listing.keys():
         if options.decode:
            (fname, num) = quoprire.subn(lsUnescapeChr, fkey)
         else:
            fname = fkey
         ls[fname] = listing[fkey]

      keys = ls.keys()
      keys.sort()

      if not sys.stdout.isatty():
         for i in keys:
            print i
      elif not options.long:
         tabulate(keys, int(os.environ.get("COLUMNS", 80)))

      else:
         totalsize = 0
         for k in keys:
            for i in range(0,len(ls[k])):
               if not ls[k][i]:
                  continue

               base = ls[k][i]
               nodes = [base['nodename']]
               isdir = (base['perms'][0] == 'd')
               islnk = (base['perms'][0] == 'l')
               criteria = ['perms', 'user', 'group']
               if isdir: 
                  base['size'] = 0
               else: 
                  criteria.append('size')
           
               # Look for duplicates from other nodes 
               for j in range(i, len(ls[k])):
                  if not ls[k][j]:
                     continue
   
                  # See if this node's instance matches the right criteria to be
                  # collapsed into a single output line
                  match = True
                  for a in criteria:
                     if base[a] != ls[k][j][a]: 
                        # Does not match
                        match = False
                        break
   
                  if match:
                     # If you specify the same file twice, we don't want
                     # the node number(s) duplicated in the node list
                     n = ls[k][j]['nodename']
                     if n not in nodes:
                        nodes.append(n)
   
                     ls[k][j] = None
   
               # Update the nodelist for this base instance
               nodes.sort(cmp=numcmp)
               base['nodename'] = nodes
   
               # Print out this file
               print self._lsline(base, k, listnodes=options.nodes)

            totalsize += base['size']

         print "%g bytes" % totalsize   
      return 

   def _lsline(self, attrs, name, listnodes=False):
      if not long: 
         return attrs['name']

      size = "%-.2g" % attrs['size']
      size = size.replace("e+", "e").replace("e0", "e")

      nodes = attrs['nodename']
      if type(nodes) != type([]):
         nodes = [nodes]

      if len(nodes) > 1:
         attrs['mtime'] = '-'
      else:
         lt = time.localtime(attrs['mtime'])
         if lt[0] == time.localtime()[0]:
            attrs['mtime'] = time.strftime("%b %d %H:%M", lt)
         else:
            attrs['mtime'] = time.strftime("%b %d  %Y", lt)
 
      if listnodes:
         if len(nodes) == len(self._Locations().locs):
            nodes = '*'
         else:
            nodes.sort(cmp=numcmp)
            nodes = string.join(nodes,',')
      else:
         nodes = len(nodes)

      return "%s %7s %8s %8s %6s %12s %s" % (attrs['perms'], nodes, attrs['user'], attrs['group'], size, attrs['mtime'], name)

   def rm(self, args):
      """Delete a file or directory tree in the cloud.
Remove empty file or directory.  Normal rm flags are passed through; -f is implied.
"""
      p = MethodOptionParser(fixed=["files..."])
      p.add_option('-R', '--recurse', action='store_true', help="recurse")
      (options, args) = p.parse_args(args)
      if options.recurse:
         options = ["-R"]
      else:
         options = []

      # Assumes shared space
      self._Locations().forAll(["rm", "-f"] + options, args).collect()
      self._Locations().pick(n=1).forAll(["rm", "-f"] + options + ["%(SYNCDIR)/" + f for f in args], subst=True).collect()


   def init(self, args):
      """Create/correct directory structure.
 If a groupname is specified, it will be applied to the directories."""
      (options, args) = MethodOptionParser(fixed=["[group]"]).parse_args(args)

      self.mkdir(["/", "/jobs", "/reduce", "/sys"], absolutes=["%(SYNCDIR)"], async=True)
      if args:
         self.chgrp([args[0], "/", "/jobs", "/reduce", "/sys"], absolutes=["%(SYNCDIR)"])
      self.chmod(["g+rwxs", "/", "/jobs", "/reduce"], absolutes=["%(SYNCDIR)"], ignoreErrors=True)
      self._Locations().put([__file__], "/sys/fm").collect()

   def _MapComponent(self, cmd, inglobs, outglobs, options, reduce=False, parent=None, tmpdir="/tmp/", procsPerCpu=None):
      jobdesc = "[mrjob]\ncmd = %s\ninputs = %s\n" % (repr(cmd), string.join(inglobs))
      if parent: 
         jobdesc += "parent = %s\n" % (parent)

      if procsPerCpu: 
         jobdesc += "procspercpu = %s\n" % (procsPerCpu)

      if options.continuous:
         jobdesc += "continuous = True\n"

      if reduce:
         jobdesc += "reduce = True\n"

      h = printableHash(jobdesc)
      jobfilename = tmpdir + h
      jobfile = open(jobfilename, "w")
      jobfile.write(jobdesc)
      jobfile.close()

      # Each component of pipeline uses previous output as input
      if reduce:
         newinglobs = ["/reduce/" + h + "/*"]
         outglobs += newinglobs
      else:
         newinglobs = [i + ".d/" + escape(cmd) for i in inglobs]
         outglobs += newinglobs

      if options.fresh:
         #XXX lower latency to accumulate these up to map() and let it do a bulk delete

         if reduce:
            syncfiles = ["%(SYNCDIR)/reduce/" + h + "/*"]
         else:
            syncfiles = ["%(SYNCDIR)/" + g + ".d/" + escape(cmd) + "/status" for g in inglobs] 

         # Delete sync files so that computation is forced to (re-)execute
         # These files are shared, so we pick a minimum number of nodes to do this on
         self._Locations().pick().forAll(["rm", "-f"], syncfiles, absolute=True, subst=True).collect()

         # Also delete previous output so that it doesn't keep getting used
         # (In the case of replication, a node may not replace the old output with new output
         self._Locations().forAll(["rm", "-Rf"], newinglobs).collect()

         # Only force the first stage of the pipeline.  Out-of-date checks will handle downstream.
         options.fresh = False

      newinglobs = [i + "/*" for i in newinglobs]
      return h, newinglobs

class SchedLock:
   """Traditional daemon-style lock as a file containing the pid of the holder"""
   def __init__(self, rootdir):
      self.lockname = rootdir + "/.fmschedlock-" + str(os.getuid())

   def checkpid(self):
      # Check for running
      try:
         f = open(self.lockname)
      except:
         return False

      pid = f.read().strip()
      if not pid: 
         print >>sys.stderr, "Warning, ignoring empty lockfile", self.lockname
         return False
      pid = int(pid)

      try:
          # This will do nothing, but make sure the proc is running and ours
          os.kill(pid, 0)

          # If we got here, then the process exists, so give up
          return pid
      except OSError, e:
          if e.errno == errno.ESRCH:
             # process not running
             return None

          else:
             # Some other unexpected error
             raise
      
   def lock(self):
      try:
         fd = os.open(self.lockname, os.O_CREAT|os.O_SYNC|os.O_EXCL|os.O_WRONLY)
         lockfile = os.fdopen(fd, "w")
         lockfile.write(str(os.getpid()))
         lockfile.close()
         return True

      except OSError, e:
         if e.errno == errno.EEXIST:
            # Check for running
            if self.checkpid():
               return False
            else:
               # Stale, so Steal
               tmp = self.lockname + "-" + str(os.getpid())
               try:
                  os.unlink(self.lockname)
               except:
                  # Somebody beat us
                  return False
               
               # XXX race here if 2 procs competing to steal and 1 unlinks and recreates before 2nd unlinks
               time.sleep(1)

               f = file(tmp, "w")
               f.write(str(os.getpid()))
               f.close()
               try:
                  os.rename(tmp, self.lockname)
                  return True
               except OSError, e:
                  if e.errno == errno.EEXIST:
                     # Somebody beat us
                     return False
                  else:
                     raise
         else:
            raise

   def unlock(self):
      #XXX: doesn't check that we still hold the lock
      os.unlink(self.lockname)
      
class FmLocalCommands(CommandSetBase):
   def __init__(self, args):
      self._locs = None
      p = MethodOptionParser()
      p.set_usage("""fm _local command args...\n""" + self._usage())
      p.add_option("-n", "--node", help="Node name for this node")
      (options, args) = p.parse_args(args)

      if options.node:
         # If -n is given, then we are a direct slave and 
         # should get our config on stdin
         os.environ['FMNODE'] = options.node
         self.Locations = FmLocations(stdin=True)
      else:
         # We are a child of a scheduler and
         # should pick-up the config from an environment variable
         options.node = os.environ['FMNODE']
         self.Locations = FmLocations()

      self.Locations.thisis(options.node)

      CommandSetBase.__init__(self, args, p)

   def replicate(self, args):
      procs = Procs(retries=7)
      errors,ignore = self.Locations.put(args, "/", procs=procs, relative=True, pickn=True, all2all=True).collect()
      return errors

   def redistribute(self, args):
      procs = Procs(retries=7)
      errors,ignore = self.Locations.put(args, "/", procs=procs, relative=True, pickn=1, all2all=True, hashbasename=True).collect()
      return errors

   def _lscharmod(self, perms, offset, chr):
      perms = list(perms)
      if perms[offset] == "-": 
           perms[offset] = chr.upper()
      else:
           perms[offset] = chr.lower()
      return string.join(perms, '')

   def lsone(self, fname, fs):
      # Construct the dict that is pickled and passed to the client
      d = {}
      d['name'] = fname
      #d['node'] = self.Locations.thisname
      #d['nodename'] = self.Locations.this.name

      if not fs:
          d['user'] = '?'
          d['group'] = '?'
          d['size'] = -1
          d['mtime'] = 0
          d['perms'] = '----------'

          # No stat structure
          return d
      
      # Convert to ls-style human-readable permission mask
      perms = "?"
      for kind in "BLK", "CHR", "DIR", "LNK", "SOCK", "FIFO", "REG":
         if getattr(stat, "S_IS"+kind)(fs.st_mode):
            perms = kind[0].lower().replace("f","p")
      if perms == "r":
         perms = "-"
         
      for who in "USR", "GRP", "OTH":
         for what in "R", "W", "X":
            if fs.st_mode & getattr(stat,"S_I"+what+who):
               c = what.lower()
            else:
               c = "-"
            perms += c

      if stat.S_ISUID & fs.st_mode:
         perms = self._lscharmod(perms, 3, "s")
      if stat.S_ISGID & fs.st_mode:
         perms = self._lscharmod(perms, 6, "s")
      if stat.S_ISVTX & fs.st_mode:
         perms = self._lscharmod(perms, 9, "t")
 
      d['perms'] = perms
      d['size'] = fs.st_size
      d['mtime'] = fs.st_mtime

      try:
         d['user'] = pwd.getpwuid(fs.st_uid).pw_name
      except:
         d['user'] = str(fs.st_uid)

      try:
         d['group'] = grp.getgrgid(fs.st_gid).gr_name
      except:
         d['group'] = fs.st_gid

      return d

   def localstats(self, args):
      os.chdir(self.Locations.this.rootdir)  
      args = multiglob(["./" + a for a in args])
      statlist = []
      for apath in args:
         stats = cPickle.load(open(apath))
         stats['statusfile'] = apath
         statlist.append(stats)

      sys.stdout.write(cPickle.dumps(statlist))
      return 

   def localjobs(self, args):
      jobs = {}
      jobdir = self.Locations.this.jobdir
      for j in os.listdir(self.Locations.this.jobdir):
         jobs[j] = FmJob(jobdir + "/" + j)
         jobuid = os.stat(jobdir + "/" + j).st_uid
         jobs[j].uid = jobuid

      sys.stdout.write(cPickle.dumps(jobs))
      return
      
   def localls(self, args):
      os.chdir(self.Locations.this.rootdir)  
      #args = [(a.lstrip("/")).rstrip("/") for a in args] # Make relative; even if it started with a /
      procs = Procs()
      args = multiglob(["./" + a for a in args])
      if not args:
         return
      output = []
      for apath in args:

         # aname is the virtual name of the file; will be shown to user.
         # So strip the leading ./ that we added to make it relative
         aname = apath[2:].rstrip('/')

         s = os.stat(apath)
         if stat.S_ISDIR(s.st_mode):
            for f in os.listdir(apath):
               fpath = apath + "/" + f
               fname = aname + "/" + f
               s = None
               try:
                  s = os.lstat(fpath)
               except:
                  pass

               output.append(self.lsone(fname, s))
                
         else:
            output.append(self.lsone(aname, s))
            
      sys.stdout.write(cPickle.dumps(output))
      return

   def sched(self, args=[]):
      """Start a scheduler for this node."""

      sl = SchedLock(self.Locations.this.rootdir)
      s = JobScheduler(self.Locations.this)

      f = file(os.devnull)
      os.dup2(f.fileno(), 0)

      # daemonify
      if not os.fork():
         # Middle-child
         os.setsid()
         if not os.fork():
            # Child

            f = file(self.Locations.this.rootdir + "/.schedlog-" + str(os.getuid()), 'w')
            os.dup2(f.fileno(), 1)
            os.dup2(f.fileno(), 2)

            # For now, the scheduler and FmJob still assume a global Locations var
            global Locations
            Locations = self.Locations
      
            if not sl.lock(): 
               # Somebody else running
               return 
      
            try:
               s.RunUntilDone()
            except:
               # Try very hard not leave a stale lockfile
               sl.unlock()
               raise
            sl.unlock()
            return
         else:
            self.Locations.cleanup = None #Don't cleanup in parent; child will

         sys.exit(0) # Children should just exit now

      self.Locations.cleanup = None #Don't cleanup in parent; child will


   def wait(self, args):
      """Wait for a specified jobid to complete (and make sure scheduler is running)"""
      self.sched()
      jobid = args[0]
      nextcmd = args[1:]

      jobfile = self.Locations.this.rootdir+"/jobs/"+jobid
      #print >>sys.stderr, "Waiting on", jobfile

      sl = SchedLock(self.Locations.this.rootdir)

      delay = 0.1
      # When the jobfile is gone (or if the scheduler dies prematurely), we're done
      while os.path.isfile(jobfile):
         # Make sure scheduler is running
         if not sl.checkpid():
            # Could have just finished, so make sure jobfile didn't just get deleted
            if os.path.isfile(jobfile):
               # Delay and check again in case the scheduler was just starting
               time.sleep(2)
               if not sl.checkpid() and os.path.isfile(jobfile):
                  print >>sys.stderr, "Error: scheduler exited while job still running.  Dumping log:"
                  sys.stderr.writelines(file(self.Locations.this.rootdir + "/.schedlog-" + str(os.getuid())))
                  return 1

         # Wait 
         # XXX: use famd
         time.sleep(delay)
         delay *= 2
         if delay > 1:
            delay = 1

      # Successful completion
   
      # Do anything else 
      if nextcmd:
         if nextcmd[0] == "redistribute":
            return self.redistribute(nextcmd[1:])
         else:
            print >>sys.stderr, "Unknown command following wait: ", nextcmd

if __name__ == "__main__":
   FmCommands(sys.argv[1:])
